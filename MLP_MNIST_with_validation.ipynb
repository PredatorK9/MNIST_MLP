{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_MNIST_with_validation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_w8EYMYzEBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing all the required libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rD19NB1zsLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "# defining our transforms\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# getting the data\n",
        "train_data = datasets.MNIST('data', train=True, download=True,\n",
        "                            transform=transform)\n",
        "test_data = datasets.MNIST('data', train=False, download=True,\n",
        "                            transform=transform)\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "                                           sampler=train_sampler,\n",
        "                                           num_workers=num_workers)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "                                           sampler=valid_sampler,\n",
        "                                           num_workers=num_workers)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
        "                                           num_workers=num_workers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI12E7A25A51",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "18bfa801-1ccc-4534-fa95-008c10791342"
      },
      "source": [
        "# define the structure of the neural net\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(28 * 28, 512)\n",
        "    self.fc2 = nn.Linear(512, 512)\n",
        "    self.fc3 = nn.Linear(512, 10)\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "  # forward method of the class\n",
        "  # it defines how the input will be passed through the different layers\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 28 * 28)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "model = Net()\n",
        "print(model)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJSD_Z_l6_Hq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss function (categorical loss entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# optimizer function (Stochastic Gradient Descent with learn rate = 0.01)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhSHUJDY7i0g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 924
        },
        "outputId": "ef0cffd7-639a-4c11-e503-56e296ac7c82"
      },
      "source": [
        "# create the training and the validation loop\n",
        "EPOCHS = 50\n",
        "valid_loss_min = np.Inf # giving validation loss 'Infinity'\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  train_loss = 0.0\n",
        "  valid_loss = 0.0\n",
        "\n",
        "  # set the model in train mode for train pass\n",
        "  model.train()\n",
        "\n",
        "  for features, labels in train_loader:\n",
        "    # clear the gradients of all optimized variable\n",
        "    optimizer.zero_grad()\n",
        "    # make a forward pass: compute the predicted values by giving inputs\n",
        "    output = model(features)\n",
        "    # calculate the loss\n",
        "    loss = criterion(output, labels)\n",
        "    # backward pass: compute gradient of loss with respect to models parameters\n",
        "    loss.backward()\n",
        "    # perform a single optimization step\n",
        "    optimizer.step()\n",
        "    # update training loss\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  # set the model in eval mode for validation pass\n",
        "  model.eval()\n",
        "  for feature, labels in valid_loader:\n",
        "    output = model(features)\n",
        "    # calculate the loss\n",
        "    loss = criterion(output, labels)\n",
        "    # update the validation loss\n",
        "    valid_loss = loss.item()\n",
        "\n",
        "  train_loss = train_loss / len(train_loader)\n",
        "  valid_loss = valid_loss / len(valid_loader)\n",
        "\n",
        "  print(f\"Epoch :{epoch + 1:2d}/{EPOCHS}\\t Training loss : {train_loss:.6f}\"\n",
        "        f\"\\tValidation loss: {valid_loss:.6f}\")\n",
        "  \n",
        "  # save the model if validation loss has decreased\n",
        "  if valid_loss <= valid_loss_min:\n",
        "    print(f\"Validation loss decreased ({valid_loss_min:.6f}) ------>\"\n",
        "          f\"({valid_loss:.6f})\\t Saving Model....\\n\")\n",
        "    torch.save(model.state_dict(), 'model.pth') \n",
        "    valid_loss_min = valid_loss"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 1/50\t Training loss : 0.216909\tValidation loss: 0.017518\n",
            "Validation loss decreased (inf) ------>(0.017518)\t Saving Model....\n",
            "\n",
            "Epoch : 2/50\t Training loss : 0.184629\tValidation loss: 0.013592\n",
            "Validation loss decreased (0.017518) ------>(0.013592)\t Saving Model....\n",
            "\n",
            "Epoch : 3/50\t Training loss : 0.163380\tValidation loss: 0.016941\n",
            "Epoch : 4/50\t Training loss : 0.145481\tValidation loss: 0.015427\n",
            "Epoch : 5/50\t Training loss : 0.130361\tValidation loss: 0.014969\n",
            "Epoch : 6/50\t Training loss : 0.117077\tValidation loss: 0.014717\n",
            "Epoch : 7/50\t Training loss : 0.106629\tValidation loss: 0.016813\n",
            "Epoch : 8/50\t Training loss : 0.099433\tValidation loss: 0.016447\n",
            "Epoch : 9/50\t Training loss : 0.090074\tValidation loss: 0.017203\n",
            "Epoch :10/50\t Training loss : 0.084577\tValidation loss: 0.018346\n",
            "Epoch :11/50\t Training loss : 0.078516\tValidation loss: 0.018851\n",
            "Epoch :12/50\t Training loss : 0.072886\tValidation loss: 0.015333\n",
            "Epoch :13/50\t Training loss : 0.069211\tValidation loss: 0.021249\n",
            "Epoch :14/50\t Training loss : 0.063402\tValidation loss: 0.020280\n",
            "Epoch :15/50\t Training loss : 0.059413\tValidation loss: 0.020858\n",
            "Epoch :16/50\t Training loss : 0.056188\tValidation loss: 0.019307\n",
            "Epoch :17/50\t Training loss : 0.052763\tValidation loss: 0.015605\n",
            "Epoch :18/50\t Training loss : 0.050056\tValidation loss: 0.018639\n",
            "Epoch :19/50\t Training loss : 0.047551\tValidation loss: 0.021591\n",
            "Epoch :20/50\t Training loss : 0.042954\tValidation loss: 0.023991\n",
            "Epoch :21/50\t Training loss : 0.042446\tValidation loss: 0.021988\n",
            "Epoch :22/50\t Training loss : 0.039424\tValidation loss: 0.022102\n",
            "Epoch :23/50\t Training loss : 0.036813\tValidation loss: 0.018035\n",
            "Epoch :24/50\t Training loss : 0.035483\tValidation loss: 0.024472\n",
            "Epoch :25/50\t Training loss : 0.033645\tValidation loss: 0.026963\n",
            "Epoch :26/50\t Training loss : 0.032158\tValidation loss: 0.020152\n",
            "Epoch :27/50\t Training loss : 0.030228\tValidation loss: 0.019083\n",
            "Epoch :28/50\t Training loss : 0.029050\tValidation loss: 0.022689\n",
            "Epoch :29/50\t Training loss : 0.028029\tValidation loss: 0.023204\n",
            "Epoch :30/50\t Training loss : 0.026945\tValidation loss: 0.022579\n",
            "Epoch :31/50\t Training loss : 0.025333\tValidation loss: 0.027532\n",
            "Epoch :32/50\t Training loss : 0.023880\tValidation loss: 0.022484\n",
            "Epoch :33/50\t Training loss : 0.022398\tValidation loss: 0.019113\n",
            "Epoch :34/50\t Training loss : 0.022280\tValidation loss: 0.024440\n",
            "Epoch :35/50\t Training loss : 0.021179\tValidation loss: 0.020796\n",
            "Epoch :36/50\t Training loss : 0.020912\tValidation loss: 0.022526\n",
            "Epoch :37/50\t Training loss : 0.019253\tValidation loss: 0.025968\n",
            "Epoch :38/50\t Training loss : 0.018527\tValidation loss: 0.026025\n",
            "Epoch :39/50\t Training loss : 0.017785\tValidation loss: 0.033438\n",
            "Epoch :40/50\t Training loss : 0.017737\tValidation loss: 0.026677\n",
            "Epoch :41/50\t Training loss : 0.016432\tValidation loss: 0.029153\n",
            "Epoch :42/50\t Training loss : 0.015515\tValidation loss: 0.028184\n",
            "Epoch :43/50\t Training loss : 0.015768\tValidation loss: 0.018829\n",
            "Epoch :44/50\t Training loss : 0.015051\tValidation loss: 0.023148\n",
            "Epoch :45/50\t Training loss : 0.014041\tValidation loss: 0.029524\n",
            "Epoch :46/50\t Training loss : 0.013932\tValidation loss: 0.027083\n",
            "Epoch :47/50\t Training loss : 0.012897\tValidation loss: 0.023126\n",
            "Epoch :48/50\t Training loss : 0.013513\tValidation loss: 0.026711\n",
            "Epoch :49/50\t Training loss : 0.012042\tValidation loss: 0.027913\n",
            "Epoch :50/50\t Training loss : 0.011999\tValidation loss: 0.024917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3gpQMpoCLmW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3aeced6-b51a-44cb-919e-110a4851c7a8"
      },
      "source": [
        "# load the model with lowest validation loss\n",
        "model.load_state_dict(torch.load('model.pth'))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPdRRiiMCXyb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "0a1e0ddd-5af6-4840-ccb9-480528d8ce9b"
      },
      "source": [
        "# initialize lists to monitor test loss and accuracy\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for data, target in test_loader:\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)\n",
        "    # compare predictions to true label\n",
        "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(len(target)):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# calculate and print avg test loss\n",
        "test_loss = test_loss/len(test_loader.sampler)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            str(i), 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.155779\n",
            "\n",
            "Test Accuracy of     0: 98% (968/980)\n",
            "Test Accuracy of     1: 98% (1115/1135)\n",
            "Test Accuracy of     2: 94% (976/1032)\n",
            "Test Accuracy of     3: 97% (980/1010)\n",
            "Test Accuracy of     4: 95% (935/982)\n",
            "Test Accuracy of     5: 91% (813/892)\n",
            "Test Accuracy of     6: 95% (917/958)\n",
            "Test Accuracy of     7: 95% (980/1028)\n",
            "Test Accuracy of     8: 93% (914/974)\n",
            "Test Accuracy of     9: 93% (948/1009)\n",
            "\n",
            "Test Accuracy (Overall): 95% (9546/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}